name: Reliable Call Transcription (1000+ Recordings)

on:
  schedule:
    # Run every day at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      target_date:
        description: 'Date to process (YYYY-MM-DD). Leave empty for yesterday.'
        required: false
        type: string

# needed to upload the Excel to a Release
permissions:
  contents: write

jobs:
  transcribe-calls:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        # add Excel libs
        pip install pandas openpyxl

    - name: Run reliable transcription
      env:
        RC_CLIENT_ID: ${{ secrets.RC_CLIENT_ID }}
        RC_CLIENT_SECRET: ${{ secrets.RC_CLIENT_SECRET }}
        RC_JWT: ${{ secrets.RC_JWT }}
        GROQ_API_KEY_1: ${{ secrets.GROQ_API_KEY_1 }}
        GROQ_API_KEY_2: ${{ secrets.GROQ_API_KEY_2 }}
        GROQ_API_KEY_3: ${{ secrets.GROQ_API_KEY_3 }}
        GROQ_API_KEY_4: ${{ secrets.GROQ_API_KEY_4 }}
        GROQ_API_KEY_5: ${{ secrets.GROQ_API_KEY_5 }}
        GROQ_API_KEY_6: ${{ secrets.GROQ_API_KEY_6 }}
        TARGET_DATE: ${{ inputs.target_date }}
        # Ultra-conservative settings to avoid RC limits
        RC_RPS: 0.25
        RC_MEDIA_DELAY: 12
      run: |
        python reliable_processor.py

    - name: Upload results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: transcription-results-${{ github.run_number }}
        path: daily_recordings/
        retention-days: 30

    - name: Show summary
      if: always()
      run: |
        echo "=== TRANSCRIPTION COMPLETE ==="
        if [ -d "daily_recordings" ]; then
          echo "Results saved in daily_recordings/"
          find daily_recordings -name "*.json" -exec echo "Found: {}" \;
          if [ -f daily_recordings/*/summary.json ]; then
            echo "=== SUMMARY ==="
            cat daily_recordings/*/summary.json
          fi
        fi

    # ========== NEW: Set DATE_STR for file/release ==========
    - name: Set DATE_STR
      run: |
        if [ -n "${{ inputs.target_date }}" ]; then
          DATE_STR="${{ inputs.target_date }}"
        else
          DATE_STR="$(date -u +%F)"   # e.g. 2025-08-25
        fi
        echo "DATE_STR=$DATE_STR" >> $GITHUB_ENV

    # ========== NEW: Build Excel from all CSV/JSON in daily_recordings ==========
    - name: Build Excel (CSV + JSON -> data-${{ env.DATE_STR }}.xlsx)
      if: always()
      run: |
        python - << 'PY'
        import os, glob, json, re
        import pandas as pd

        base = "daily_recordings"
        out = f"data-{os.environ['DATE_STR']}.xlsx"

        if not os.path.isdir(base):
          print("No daily_recordings/ directory found; creating empty Excel.")
          with pd.ExcelWriter(out, engine="openpyxl") as w:
            pd.DataFrame([]).to_excel(w, index=False, sheet_name="EMPTY")
          raise SystemExit(0)

        def clean_sheet(name: str) -> str:
          # Excel restrictions: <=31 chars, no []:*?/\
          name = re.sub(r'[\[\]\:\*\?\/\\]', '_', name)[:31] or "Sheet"
          return name

        csvs  = sorted(glob.glob(os.path.join(base, "**", "*.csv"),  recursive=True))
        jsons = sorted(glob.glob(os.path.join(base, "**", "*.json"), recursive=True))

        used = set()
        def uniq(n: str) -> str:
          base = n[:28]
          i, s = 1, n
          while s in used:
            s = f"{base}_{i}"
            i += 1
          used.add(s)
          return s

        with pd.ExcelWriter(out, engine="openpyxl") as w:
          all_stats = []
          
          # First pass: Look for statistics in JSON files
          for p in jsons:
            try:
              with open(p, "r", encoding="utf-8") as f:
                data = json.load(f)
              
              # Check if this JSON has statistics
              if isinstance(data, dict) and 'statistics' in data:
                stats = data['statistics']
                all_stats.append({
                  "File": os.path.basename(p),
                  "Processing Date": stats.get('processing_date', ''),
                  "Total Found": stats.get('total_recordings_found', 0),
                  "Total Processed": stats.get('total_recordings_processed', 0),
                  "Total Skipped": stats.get('total_recordings_skipped', 0),
                  "Success Rate": stats.get('success_rate', ''),
                  "Audio Minutes": stats.get('total_audio_minutes', 0),
                  "Download Errors": stats.get('download_errors', 0),
                  "Transcription Errors": stats.get('transcription_errors', 0)
                })
                
                # If it has transcriptions, create a dedicated sheet
                if 'transcriptions' in data and data['transcriptions']:
                  trans_df = pd.DataFrame(data['transcriptions'])
                  if 'duration' in trans_df.columns:
                    trans_df['duration_minutes'] = round(trans_df['duration'] / 60, 1)
                  sheet = uniq(clean_sheet("Transcriptions"))
                  trans_df.to_excel(w, index=False, sheet_name=sheet)
              else:
                # Regular JSON processing
                try:
                  df = pd.json_normalize(data)
                except Exception:
                  df = pd.DataFrame({"raw_json":[json.dumps(data, ensure_ascii=False)]})
                sheet = uniq(clean_sheet(os.path.splitext(os.path.basename(p))[0] or "JSON"))
                df.to_excel(w, index=False, sheet_name=sheet)
            except Exception as e:
              print(f"JSON read failed {p}: {e}")
              continue
          
          # Create SUMMARY sheet first if we found statistics
          if all_stats:
            summary_df = pd.DataFrame(all_stats)
            summary_df.to_excel(w, index=False, sheet_name="SUMMARY")
            print(f"Added SUMMARY sheet with statistics from {len(all_stats)} files")
          
          # Process CSV files (check for stats in comments)
          for p in csvs:
            try:
              # Check for statistics in comment headers
              stats_dict = {"File": os.path.basename(p)}
              with open(p, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                  if i > 10:
                    break
                  if line.startswith('#'):
                    if 'Total Recordings Found:' in line:
                      stats_dict['Total Found'] = line.split(':')[-1].strip()
                    elif 'Total Recordings Processed:' in line:
                      stats_dict['Total Processed'] = line.split(':')[-1].strip()
                    elif 'Total Recordings Skipped' in line:
                      stats_dict['Total Skipped'] = line.split(':')[-1].strip()
                    elif 'Success Rate:' in line:
                      stats_dict['Success Rate'] = line.split(':')[-1].strip()
              
              if len(stats_dict) > 1 and not all_stats:  # Add CSV stats if no JSON stats
                all_stats.append(stats_dict)
              
              # Read the actual CSV
              df = pd.read_csv(p, comment='#')
              sheet = uniq(clean_sheet(os.path.splitext(os.path.basename(p))[0] or "CSV"))
              df.to_excel(w, index=False, sheet_name=sheet)
            except Exception as e:
              print(f"CSV read failed {p}: {e}")
              continue

        print(f"Wrote {out} with {len(csvs)} CSV sheet(s) and {len(jsons)} JSON sheet(s).")
        if all_stats:
          print("âœ… Added SUMMARY sheet with recording statistics!")
        PY

    # ========== NEW: Publish the Excel on a GitHub Release ==========
    - name: Publish Excel on Release
      if: always()
      uses: softprops/action-gh-release@v2
      with:
        tag_name: excel-${{ env.DATE_STR }}
        name: "Daily Excel - ${{ env.DATE_STR }}"
        files: data-${{ env.DATE_STR }}.xlsx
